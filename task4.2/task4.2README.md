一，创新点：对比任务三的核心改动
  1. 策略优化
   （1）Warmup 学习率预热：前 5 轮线性提升学习率，解决训练初期梯度不稳定问题，避免模型震荡发散。
      (深度学习训练中的一种学习率调整策略，核心逻辑是「先慢后快」，帮模型平稳起步：
      1. 先理解两个关键词
       学习率：控制模型参数更新的「步长」。步长太大容易「走歪」（震荡），太小又「走得慢」（收敛慢）。
       训练初期：模型刚接触数据，对规律还很陌生，梯度（参数更新的方向）往往不稳定。
      2. Warmup 做了什么？
       前 5 轮（或前几步）不直接用设定好的大学习率，而是从一个很小的学习率开始，线性地逐步增加到目标学习率。比如：目标学习率是 0.01，那第 1 轮用 0.002，第 2 轮 0.004…… 第 5 轮升到 0.01，之后再用 0.01 正常训        练。
      3. 为什么要这么做？
       训练初期模型「心里没底」，如果一开始步长太大，容易在参数空间里「乱撞」（震荡），甚至直接「走偏」（发散，训练失败）。Warmup 相当于让模型先「小步试探」，慢慢找到感觉，等梯度稳定了再「大步前进」，能让训练更        平稳。)
      （举个生活化的例子：就像开车起步，先轻踩油门慢慢提速，等车稳了再正常加速，不容易熄火或失控～）

   （2）Mixup 数据混合增强：对训练样本做线性插值，扩充数据分布，强制模型学习样本间的线性关系，缓解过拟合。
      1. Mixup 不会直接用原图 / 原数据训练，而是随机挑两个训练样本，按比例 “揉” 在一起，生成一个新样本。
       举个直观的例子（以图像分类为例）：
       样本 A：一张猫的图片，标签是「猫」（可以理解为 “猫的概率 = 1，狗的概率 = 0”）；
       样本 B：一张狗的图片，标签是「狗」（“狗的概率 = 1，猫的概率 = 0”）；
       Mixup 操作：随机选一个比例（比如 0.7），把 A 的像素值 ×0.7 + B 的像素值 ×0.3，生成一张 “70% 像猫 + 30% 像狗” 的新图；同时标签也按同样比例混合：「猫的概率 = 0.7，狗的概率 = 0.3」。
       这里的 “线性插值”，本质就是按权重加权混合（权重加起来等于 1）。
      2. 为什么要这么做？解决两个痛点
        痛点 1：数据不够 “丰富”真实训练数据往往有限，且分布比较 “窄”（比如只有 “纯猫”“纯狗”）。Mixup 通过混合样本，强行扩充了数据的 “覆盖范围”（让模型看到 “猫和狗之间的过渡状态”），相当于用少量数据造出了更         多 “虚拟样本”。
        痛点 2：模型容易 “死记硬背”（过拟合）如果只给模型看 “纯猫”“纯狗”，它可能会记住一些无关细节（比如某张猫图的背景是草地），而不是 “猫的本质特征”。Mixup 强制模型学习样本间的 “线性关系”—— 比如 “猫的特征         越多，标签越接近猫”，这会迫使模型去抓更本质、更通用的规律，而不是死记硬背特定样本。
      3. 最终效果：让模型更 “抗打”
        用 Mixup 训练后，模型在没见过的新数据上表现会更好（泛化能力更强），不容易因为数据小变化就 “翻车”。
        再举个生活化的类比：学认水果时，如果只给你看 “完整的苹果” 和 “完整的橘子”，你可能只记住 “苹果是圆的、橘子是橙的”；但如果给你看 “60% 苹果 + 40% 橘子” 的混合图，你就得去学 “形状圆润、果肉质感” 这种更         本质的特征 —— 下次遇到有点像苹果又有点像橘子的，你也能大概认出来，这就是 Mixup 的作用
        
   （3）学习率步长衰减：每 20 轮学习率减半，前期快速探索参数空间，后期精细调参，提升收敛稳定性。
        
   （4）早停策略：监控验证集准确率，连续 10 轮无提升则终止训练，保存泛化能力最强的模型，避免无效训练。
        
   （5）进阶数据增强：在任务三的随机裁剪、水平翻转基础上，新增随机旋转（±15°）和颜色抖动（亮度 / 对比度 / 饱和度），提升模型对角度、光照变化的鲁棒性。
        
  2. 结构优化
   加深卷积层：从任务三的 3 层卷积扩展为 4 层卷积，提升模型对细粒度特征的提取能力。
        
   批量归一化（BN）：每一层卷积后加入 BN 层，解决深层网络的 “内部协变量偏移” 问题，加速收敛，稳定训练过程。
   
   正则化强化：将 Dropout 率从 0.5 提高到 0.6，进一步缓解过拟合，缩小训练 / 验证集准确率差距。
  3. 工程规范
   固定随机种子：设置 seed=42，保证实验结果可复现，便于对比优化前后效果。
   
   自动日志记录：训练过程自动保存 .log 文件，完整记录每轮 Loss、准确率和学习率变化。
   （.log 文件会按时间顺序，把系统 / 软件的操作记录、错误信息、状态变化等一一记下来。主要用途有两个：
      排查问题：如果软件崩溃、系统报错，看 .log 文件，通过里面的错误信息（比如 “哪一步失败了”“为什么失败”）快速定位原因。
      分析行为：比如网站服务器的 .log 文件，会记录 “谁在什么时候访问了哪个页面”，用来分析用户行为或安全审计。）
        
   参数量与 FLOPs 计算：使用 thop 库自动计算模型参数量和计算量，平衡高性能与轻量化。
   （（1）参数量（Parameters）
      理解：模型里 “可学习的旋钮” 数量（比如神经网络中的权重、偏置）。
      特点：
       参数量多 → 模型 “记忆容量” 大，可能拟合能力强（效果好），但占内存 / 显存多（比如手机上装不下大模型）。
       参数量少 → 模型轻便，但可能 “记不住” 复杂规律（效果差）。
     （2）FLOPs（Floating Point Operations）
       理解：模型做一次预测需要的「浮点运算次数」（比如加减乘除的次数）。
       特点：
        FLOPs 高 → 模型 “工作量大”，计算慢、耗电多（比如手机上跑起来卡顿、发热）。
        FLOPs 低 → 计算快、省电，但可能 “思考不够深”（效果差）。）
    （thop 库是什么？
      thop 是一个专门针对 PyTorch 框架的 Python 工具库，作用是自动帮你算这两个指标，不用自己手动数参数、算运算量（手动算太麻烦，还容易错）。
      你把模型传给 thop，它能输出：
      这个模型有多少个参数？
      跑一次需要多少次浮点运算？）

二，改动背后的数学 / 工程原理
1. Warmup 学习率预热

原理：训练初期模型权重随机初始化，梯度方向不稳定，大学习率会导致参数更新步长过大，模型在最优值附近震荡甚至发散。通过前 N 轮线性提升学习率，让模型逐步适应数据分布，稳定梯度传播，保证深层网络的平稳收敛。
工程效果：解决了训练初期 Loss 剧烈震荡的问题，收敛轮次减少约 20%。

2. Mixup 数据混合增强

原理：对两个随机样本的图像和标签做线性插值，公式为：mixed_x=λx1​+(1−λ)x2​,mixed_y=λy1​+(1−λ)y2​其中 λ 服从 Beta (1,1) 均匀分布。该方法强制模型学习样本间的线性关系，而非记忆单个样本的噪声，提升泛化能力。
工程效果：训练集与测试集准确率差距缩小约 5%，显著缓解过拟合。（新图像 = λ 倍的 x1​ + (1−λ) 倍的 x2​）

3. 批量归一化（BN）

原理：对每一层卷积的输出做归一化，将数据分布拉回到均值 0、方差 1 的标准正态分布，解决深层网络的 “内部协变量偏移” 问题，让梯度保持在有效区间，避免梯度消失 / 爆炸。
工程效果：模型收敛速度提升 1 倍，对学习率的敏感度大幅降低，无需精细调参即可稳定收敛。

4. 早停策略

原理：模型训练后期会开始拟合训练集的噪声，导致验证集准确率停止提升甚至下降。通过监控验证集准确率，连续多轮无提升则终止训练，保存泛化能力最强的模型。
工程效果：避免无效训练，防止模型过拟合，同时节省训练时间。

三，失败复盘
   尝试的优化方案	实际结果	原因分析	改进方案
   1.盲目加深网络到 6 层卷积	测试集准确率从 81% 降至 76%，过拟合严重	CIFAR-10 仅 32×32 分辨率，6 层卷积后特征图尺寸缩小到 1×1，丢失大量语义信息；参数量激增，小数据集下易过拟合	回归 4 层轻量化卷积结构，     通     过 BN 和进阶数据增强提升特征能力，而非盲目加深
   2.Warmup 预热轮次设为 20 轮	收敛速度变慢，最终准确率仅 78%，低于基线	预热阶段学习率过低，权重更新太慢，80 轮总训练轮次内无法充分收敛	调整 warmup_epochs 为 5 轮，适配总训练轮次，平衡预热效果与收敛速度
   3.Mixup 的 alpha 设为 5.0	训练 Loss 下降缓慢，最终准确率 77%，低于基线	alpha 过大，Beta 分布集中于 0.5，样本过度混合，丢失核心语义特征，模型无法学习有效分类边界	调整 alpha 为 1.0，Beta 分布为均匀分       布，平衡样本混合程度与特征保留度
   4.学习率初始值设为 0.01	训练 Loss 震荡不收敛，准确率始终在 10% 左右（随机猜测）	学习率过大，参数更新步长超过最优值区间，模型在局部最优附近来回震荡，无法收敛	调整初始学习率为 0.001，配合 Warmup 和步长衰      减，保证训练平稳
   （一些插曲：
   比如新建一个task4.2文件夹，搞忘改一下anaconda里的文件路径啦
   搞忘把task3里的下载的data文件复制过来了
   代码报错了：比如代码里日志文件被提前关闭了，导致最后一行 print 语句尝试往已经关闭的文件里写内容，从而触发了 ValueError: I/O operation on closed file。
              解决方案：sys.stdout.close()
                       print(f"\n===== 任务四所有文件生成完成！=====")  #这两行调换一下，之前的代码先执行了 sys.stdout.close()，把日志文件关了，后面的 print 还想往里面写，就报错了
   日常网络不好，下载上传慢，中断了，解决方案：开加速器再来一遍
   不会改GitHub里的文件名，学了一下，还挺麻烦

(代码备注）
一、核心库导入与消融实验开关（1-22 行)核心作用：
导入所有依赖库，覆盖 “数据加载、模型构建、训练优化、指标计算、可视化” 全流程；
定义ENABLE_WARMUP/ENABLE_MIXUP开关，用于消融实验（对比 “加 / 不加 Warmup+Mixup” 的效果，满足对比要求）

二、随机种子固定（24-28 行）

三、训练日志自动保存（30-33 行）

四、设备自动适配（35-37 行）

五、进阶数据增强与预处理（39-50 行）实现进阶数据增强（区别于基础的 “仅归一化”），训练集加入 “随机裁剪、翻转、旋转、颜色抖动”，提升模型泛化能力

六、数据集加载与划分（52-63 行）

七、Mixup 数据混合增强核心实现（65-80 行）

八、改进的 CNN 模型定义（82-108 行）

九、参数量 + FLOPs 计算（110-116 行）

十、训练配置（118-124 行）

十一、Warmup 学习率预热 + 步长衰减实现（126-145 行）

十二、指标记录初始化（147-149 行）

十三、训练循环（151-217 行）

十四、早停策略（219-229 行）

十五、测试集最终评估（231-246 行）

十六、类别级准确率计算（248-259 行）

十七、分类报告保存（261-265 行）

十八、可视化生成（267-299 行）

十九、日志关闭与收尾（301-305 行）



